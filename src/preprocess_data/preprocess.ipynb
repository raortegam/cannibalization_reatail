{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351d3d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "make_h_exposure.py\n",
    "Script de conveniencia que llama al módulo h_exposure y guarda el CSV final.\n",
    "\"\"\"\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_path = Path().resolve()\n",
    "sys.path.append(str(notebook_path.parent.parent))\n",
    "\n",
    "from competitive_exposure import compute_h_exposure\n",
    "from src.conf import config\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ajusta las rutas según tu estructura (ejemplo):\n",
    "    TRAIN_CSV = config.DATA_DIR + \"\\\\train_v2.csv\"\n",
    "    ITEMS_CSV = config.DATA_DIR + \"\\\\items.csv\"\n",
    "    OUT_CSV   = config.DATA_DIR + \"\\\\feature_h_exposure_v2.csv\" # o \"data/features_h_exposure.csv.gz\"\n",
    "    CHUNK_ROWS = 1_500_000\n",
    "\n",
    "    # Opción A: binario = hay al menos un competidor en promo\n",
    "    compute_h_exposure(\n",
    "        train_csv=TRAIN_CSV, \n",
    "        items_csv=ITEMS_CSV,\n",
    "        out_csv=OUT_CSV,\n",
    "        chunk_rows=CHUNK_ROWS,\n",
    "        bin_mode=\"any\",          # \"any\" o \"threshold\"\n",
    "        bin_tau=0.30,            # solo aplica si bin_mode=\"threshold\"\n",
    "        keep_class_in_output=False,\n",
    "        compression=None,        # por ejemplo \"gzip\" para OUT_CSV+\".gz\"\n",
    "    )\n",
    "\n",
    "    print(f\"Listo. CSV guardado en: {OUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a37a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('d:/repos/cannibalization_reatail/.data/raw_data/feature_h_exposure.csv'),\n",
       " WindowsPath('d:/repos/cannibalization_reatail/.data/raw_data/train_v2.csv'),\n",
       " WindowsPath('d:/repos/cannibalization_reatail/.data/raw_data/items.csv'),\n",
       " WindowsPath('d:/repos/cannibalization_reatail/.data/raw_data/stores.csv'),\n",
       " WindowsPath('d:/repos/cannibalization_reatail/data/processed'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# --- Localiza el repo raíz tomando el notebook como referencia ---\n",
    "NB_DIR = Path.cwd()  # si abres el notebook desde src/preprocess_data, esto es esa carpeta\n",
    "# Si el notebook se ejecuta desde otra cwd, ajusta buscando 'src' y 'data'\n",
    "if (NB_DIR / \"preprocess.ipynb\").exists():\n",
    "    SRC_DIR = NB_DIR\n",
    "else:\n",
    "    # intenta encontrar src/preprocess_data en los ancestros\n",
    "    for p in [NB_DIR] + list(NB_DIR.parents):\n",
    "        if (p / \"src\" / \"preprocess_data\").exists():\n",
    "            SRC_DIR = p / \"src\" / \"preprocess_data\"\n",
    "            break\n",
    "    else:\n",
    "        SRC_DIR = NB_DIR  # fallback\n",
    "\n",
    "REPO_ROOT = SRC_DIR.parents[1] if SRC_DIR.name == \"preprocess_data\" else SRC_DIR.parent\n",
    "sys.path.append(str(SRC_DIR))      # para importar módulos que coloques junto al notebook\n",
    "sys.path.append(str(REPO_ROOT))    # por si dejas el módulo en la raíz del repo\n",
    "\n",
    "# --- Rutas de entrada/salida ---\n",
    "# Tu H ya calculado (probamos con y sin punto inicial)\n",
    "H_CANDIDATES = [\n",
    "    REPO_ROOT / \"data\" / \"raw_data\" / \"feature_h_exposure.csv\",\n",
    "    REPO_ROOT / \".data\" / \"raw_data\" / \"feature_h_exposure.csv\",\n",
    "]\n",
    "for _p in H_CANDIDATES:\n",
    "    if _p.exists():\n",
    "        H_CSV = _p\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"No encontré feature_h_exposure.csv en data/raw_data o .data/raw_data\")\n",
    "\n",
    "TRAIN_CSV  = REPO_ROOT / \".data\" / \"raw_data\" / \"train_v2.csv\"\n",
    "ITEMS_CSV  = REPO_ROOT / \".data\" / \"raw_data\" / \"items.csv\"\n",
    "STORES_CSV = REPO_ROOT / \".data\" / \"raw_data\" / \"stores.csv\"\n",
    "\n",
    "OUT_DIR = REPO_ROOT / \"data\" / \"processed\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "H_CSV, TRAIN_CSV, ITEMS_CSV, STORES_CSV, OUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd69864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-13 06:23:30][MainProcess][INFO] === Inicio select_pairs_and_donors (optimizado V2) ===\n",
      "[2025-10-13 06:23:30][MainProcess][INFO] Parámetros: N_WORKERS=6, CHUNK_H=4,000,000, CHUNK_TRAIN=4,000,000\n",
      "[2025-10-13 06:23:30][MainProcess][INFO] ENV sampling: SPD_SAMPLE_CI_PER_SC=50, SPD_SAMPLE_FRAC_CI=0.25, SPD_MAX_TASKS_SCORE=20000\n",
      "[2025-10-13 06:23:30][MainProcess][INFO] [PROGRESS] Inicializando insumos ...\n",
      "[2025-10-13 06:23:30][MainProcess][INFO] Leyendo items desde d:\\repos\\cannibalization_reatail\\.data\\raw_data\\items.csv ...\n",
      "[2025-10-13 06:23:30][MainProcess][INFO] Items: 4,100 filas (0.0 MB) en 0.01s\n",
      "[2025-10-13 06:23:30][MainProcess][INFO] Leyendo stores desde d:\\repos\\cannibalization_reatail\\.data\\raw_data\\stores.csv ...\n",
      "[2025-10-13 06:23:30][MainProcess][INFO] Stores: 54 filas (0.0 MB) en 0.00s\n",
      "[2025-10-13 06:23:30][MainProcess][INFO] H no contiene 'class'. Generando archivo temporal con 'class' ...\n",
      "[2025-10-13 06:23:45][MainProcess][INFO] [PROGRESS] Anexando 'class' a H: 4,000,000/125,497,040\n",
      "[2025-10-13 06:23:51][MainProcess][INFO] [PROGRESS] Anexando 'class' a H: 8,000,000/125,497,040\n",
      "[2025-10-13 06:23:57][MainProcess][INFO] [PROGRESS] Anexando 'class' a H: 12,000,000/125,497,040\n",
      "[2025-10-13 06:24:03][MainProcess][INFO] [PROGRESS] Anexando 'class' a H: 16,000,000/125,497,040\n",
      "[2025-10-13 06:24:09][MainProcess][INFO] [PROGRESS] Anexando 'class' a H: 20,000,000/125,497,040\n",
      "[2025-10-13 06:24:15][MainProcess][INFO] [PROGRESS] Anexando 'class' a H: 24,000,000/125,497,040\n",
      "[2025-10-13 06:24:58][MainProcess][INFO] [PROGRESS] Anexando 'class' a H: 52,000,000/125,497,040\n",
      "[2025-10-13 06:25:04][MainProcess][INFO] [PROGRESS] Anexando 'class' a H: 56,000,000/125,497,040\n",
      "[2025-10-13 06:25:10][MainProcess][INFO] [PROGRESS] Anexando 'class' a H: 60,000,000/125,497,040\n",
      "[2025-10-13 06:25:16][MainProcess][INFO] [PROGRESS] Anexando 'class' a H: 64,000,000/125,497,040\n",
      "[2025-10-13 06:25:22][MainProcess][INFO] [PROGRESS] Anexando 'class' a H: 68,000,000/125,497,040\n",
      "[2025-10-13 06:25:28][MainProcess][INFO] [PROGRESS] Anexando 'class' a H: 72,000,000/125,497,040\n",
      "[2025-10-13 06:26:17][MainProcess][INFO] [PROGRESS] Anexando 'class' a H: 104,000,000/125,497,040\n",
      "[2025-10-13 06:26:23][MainProcess][INFO] [PROGRESS] Anexando 'class' a H: 108,000,000/125,497,040\n",
      "[2025-10-13 06:26:29][MainProcess][INFO] [PROGRESS] Anexando 'class' a H: 112,000,000/125,497,040\n",
      "[2025-10-13 06:26:35][MainProcess][INFO] [PROGRESS] Anexando 'class' a H: 116,000,000/125,497,040\n",
      "[2025-10-13 06:26:41][MainProcess][INFO] [PROGRESS] Anexando 'class' a H: 120,000,000/125,497,040\n",
      "[2025-10-13 06:26:47][MainProcess][INFO] [PROGRESS] Anexando 'class' a H: 124,000,000/125,497,040\n",
      "[2025-10-13 06:26:49][MainProcess][INFO] [PROGRESS] Anexando 'class' a H: 125,497,040/125,497,040\n",
      "[2025-10-13 06:26:49][MainProcess][INFO] Agregando métricas por (store,item) desde H ...\n",
      "[2025-10-13 06:27:05][MainProcess][INFO] [PROGRESS] H procesadas: 12,000,000/125,497,040\n",
      "[2025-10-13 06:27:07][MainProcess][INFO] [PROGRESS] H procesadas: 16,000,000/125,497,040\n",
      "[2025-10-13 06:27:12][MainProcess][INFO] [PROGRESS] H procesadas: 28,000,000/125,497,040\n",
      "[2025-10-13 06:27:14][MainProcess][INFO] [PROGRESS] H procesadas: 32,000,000/125,497,040\n",
      "[2025-10-13 06:27:19][MainProcess][INFO] [PROGRESS] H procesadas: 44,000,000/125,497,040\n",
      "[2025-10-13 06:27:21][MainProcess][INFO] [PROGRESS] H procesadas: 48,000,000/125,497,040\n",
      "[2025-10-13 06:27:26][MainProcess][INFO] [PROGRESS] H procesadas: 60,000,000/125,497,040\n",
      "[2025-10-13 06:27:28][MainProcess][INFO] [PROGRESS] H procesadas: 64,000,000/125,497,040\n",
      "[2025-10-13 06:27:33][MainProcess][INFO] [PROGRESS] H procesadas: 76,000,000/125,497,040\n",
      "[2025-10-13 06:27:35][MainProcess][INFO] [PROGRESS] H procesadas: 80,000,000/125,497,040\n",
      "[2025-10-13 06:27:42][MainProcess][INFO] [PROGRESS] H procesadas: 96,000,000/125,497,040\n",
      "[2025-10-13 06:27:43][MainProcess][INFO] [PROGRESS] H procesadas: 100,000,000/125,497,040\n",
      "[2025-10-13 06:27:49][MainProcess][INFO] [PROGRESS] H procesadas: 112,000,000/125,497,040\n",
      "[2025-10-13 06:27:50][MainProcess][INFO] [PROGRESS] H procesadas: 116,000,000/125,497,040\n",
      "[2025-10-13 06:27:58][MainProcess][INFO] [PROGRESS] Listo en 68.67s\n",
      "[2025-10-13 06:27:58][MainProcess][INFO] Agregando métricas de onpromotion por (store,item) ...\n",
      "[2025-10-13 06:28:05][MainProcess][INFO] [PROGRESS] train procesado: 4,000,000/59,038,132\n",
      "[2025-10-13 06:28:08][MainProcess][INFO] [PROGRESS] train procesado: 12,000,000/59,038,132\n",
      "[2025-10-13 06:28:09][MainProcess][INFO] [PROGRESS] train procesado: 16,000,000/59,038,132\n",
      "[2025-10-13 06:28:13][MainProcess][INFO] [PROGRESS] train procesado: 24,000,000/59,038,132\n",
      "[2025-10-13 06:28:14][MainProcess][INFO] [PROGRESS] train procesado: 28,000,000/59,038,132\n",
      "[2025-10-13 06:28:17][MainProcess][INFO] [PROGRESS] train procesado: 36,000,000/59,038,132\n",
      "[2025-10-13 06:28:19][MainProcess][INFO] [PROGRESS] train procesado: 40,000,000/59,038,132\n",
      "[2025-10-13 06:28:22][MainProcess][INFO] [PROGRESS] train procesado: 48,000,000/59,038,132\n",
      "[2025-10-13 06:28:24][MainProcess][INFO] [PROGRESS] train procesado: 52,000,000/59,038,132\n",
      "[2025-10-13 06:28:26][MainProcess][INFO] [PROGRESS] Listo en 28.35s\n",
      "[2025-10-13 06:28:26][MainProcess][INFO] [PROGRESS] Construyendo universo base y filtros ...\n",
      "[2025-10-13 06:28:26][MainProcess][INFO] Base filtrada: 123,535 (store,item) válidos.\n",
      "[2025-10-13 06:28:29][MainProcess][INFO] Candidatos i tras muestreo: 22,908\n",
      "[2025-10-13 06:28:29][MainProcess][INFO] Candidatos i truncados por SPD_MAX_TASKS_SCORE=20000: 20,000\n",
      "[2025-10-13 06:28:29][MainProcess][INFO] Candidatos i: 20,000 (criterios p_promo en [0.03, 0.25])\n",
      "[2025-10-13 06:28:29][MainProcess][INFO] [PROGRESS] Preparando shards de H por tienda ...\n",
      "[2025-10-13 06:28:29][MainProcess][INFO] Shards por tienda ya existen; se reutilizarán.\n",
      "[2025-10-13 06:28:29][MainProcess][INFO] [PROGRESS] Scoring caníbales (ΔH_cls) ...\n",
      "[2025-10-13 16:22:59][MainProcess][INFO] [PROGRESS] Caníbales procesados (aprox ítems): 797/20000\n",
      "[2025-10-13 16:26:46][MainProcess][INFO] [PROGRESS] Caníbales procesados (aprox ítems): 1205/20000\n",
      "[2025-10-13 16:26:51][MainProcess][INFO] [PROGRESS] Caníbales procesados (aprox ítems): 1600/20000\n"
     ]
    }
   ],
   "source": [
    "# Importa el módulo (colócalo en src/preprocess_data o en la raíz del repo)\n",
    "import select_pairs_and_donors as spd\n",
    "\n",
    "# (Opcional) sobreescribe parámetros sin tocar el archivo\n",
    "spd.N_CANNIBALS = 4\n",
    "spd.N_VICTIMS_PER_I = 5\n",
    "\n",
    "spd.N_DONORS_PER_J = 20\n",
    "spd.PRE_DAYS, spd.PRE_GAP, spd.TREAT_MAX, spd.POST_DAYS = 60, 7, 14, 30\n",
    "spd.MIN_ITEM_OBS, spd.H_SD_MIN = 200, 0.004\n",
    "spd.P_ANY_MIN, spd.P_ANY_MAX = 0.02, 0.98\n",
    "spd.P_PROMO_I_MIN, spd.P_PROMO_I_MAX, spd.P_PROMO_J_MAX = 0.03, 0.25, 0.10\n",
    "\n",
    "pairs_path, donors_path = spd.select_pairs_and_donors(\n",
    "    H_csv=str(H_CSV),\n",
    "    train_csv=str(TRAIN_CSV),\n",
    "    items_csv=str(ITEMS_CSV),\n",
    "    stores_csv=str(STORES_CSV),\n",
    "    outdir=str(OUT_DIR),\n",
    "    \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec75c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En tu script principal:\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "notebook_path = Path().resolve()\n",
    "sys.path.append(str(notebook_path.parent.parent))\n",
    "\n",
    "from src.conf import config\n",
    "from src.preprocess_data.feature_engineering import build_and_chunk\n",
    "\n",
    "\n",
    "\n",
    "raw = {\n",
    "    \"ventas\": pd.read_csv(config.DATA_DIR + \"\\\\train.csv\"),\n",
    "    \"items\": pd.read_csv(config.DATA_DIR + \"\\\\items.csv\"),\n",
    "    \"stores\": pd.read_csv(config.DATA_DIR + \"\\\\stores.csv\"),\n",
    "    \"trans\": pd.read_csv(config.DATA_DIR + \"\\\\transactions.csv\"),\n",
    "    \"oil\": pd.read_csv(config.DATA_DIR + \"\\\\oil.csv\"),\n",
    "    \"hol\": pd.read_csv(config.DATA_DIR + \"\\\\holidays_events.csv\"),\n",
    "}\n",
    "chunks = build_and_chunk(raw)  # {'A': dfA, 'B': dfB, 'C': dfC, 'merged_core_only': df}\n",
    "\n",
    "# Panel diario completo (núcleos recombinados, sin inconsistencias)\n",
    "panel_daily = chunks[\"merged_core_only\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130c0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(notebook_path.parent.parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02bdec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec95b7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_path = Path().resolve()\n",
    "sys.path.append(str(notebook_path.parent.parent))\n",
    "from src.preprocess_data.competitive_exposure import build_competitive_exposure\n",
    "import pandas as pd\n",
    "from src.conf import config\n",
    "import os\n",
    "\n",
    "# 1) Panel base\n",
    "# panel = build_full_panel(raw)\n",
    "panel = pd.read_csv(os.path.join(config.OUTPUT_DIR, \"panel_semanal.csv\"))\n",
    "\n",
    "# 2) Exposición competitiva (ahora sí tendrás E_cat_isw y E_bin_isw)\n",
    "panel = build_competitive_exposure(\n",
    "    panel,\n",
    "    scope_col=\"family\",          # o \"class\" si prefieres más granular\n",
    "    exclude_self=True,           # canibalización \"pura\"\n",
    "\n",
    "    \n",
    "    theta_E=config.THETA_E_CAT,  # p.ej., 0.2\n",
    "    compute_delta_version=True   # añade E_cat_delta_isw además de E_cat_isw\n",
    ")\n",
    "\n",
    "panel.to_csv(os.path.join(config.OUTPUT_DIR, \"panel_semanal_competitive.csv\"), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
