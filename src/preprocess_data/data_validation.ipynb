{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5305d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_validation.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_path = Path().resolve()\n",
    "sys.path.append(str(notebook_path.parent.parent))\n",
    "\n",
    "from src.conf import config\n",
    "from src.utils.utils import safe_bool_to_int, add_week_start\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Infraestructura ligera para issues y reporte\n",
    "# ==========================================================\n",
    "\n",
    "@dataclass\n",
    "class DQIssue:\n",
    "    dataset: str\n",
    "    check: str\n",
    "    severity: str  # \"ERROR\" | \"WARNING\" | \"INFO\"\n",
    "    n_affected: int\n",
    "    pct_affected: float\n",
    "    description: str\n",
    "    suggestion: str\n",
    "    sample_index: Optional[List] = None  # índices/llaves afectadas (muestra)\n",
    "    extra: Optional[Dict] = None         # metadatos opcionales\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        d = asdict(self)\n",
    "        # Redondeos amigables\n",
    "        d[\"pct_affected\"] = float(np.round(d[\"pct_affected\"], 6))\n",
    "        return d\n",
    "\n",
    "\n",
    "class DQReport:\n",
    "    def __init__(self):\n",
    "        self.issues: List[DQIssue] = []\n",
    "        self.summary: Dict[str, Dict[str, int]] = {}  # filas por dataset\n",
    "\n",
    "    def add_issue(self, issue: DQIssue):\n",
    "        self.issues.append(issue)\n",
    "\n",
    "    def set_dataset_size(self, dataset: str, n_rows: int):\n",
    "        self.summary.setdefault(dataset, {})[\"rows\"] = int(n_rows)\n",
    "\n",
    "    def to_dataframe(self) -> pd.DataFrame:\n",
    "        if not self.issues:\n",
    "            return pd.DataFrame(columns=[\n",
    "                \"dataset\",\"check\",\"severity\",\"n_affected\",\"pct_affected\",\"description\",\"suggestion\",\"sample_index\",\"extra\"\n",
    "            ])\n",
    "        return pd.DataFrame([i.to_dict() for i in self.issues])\n",
    "\n",
    "    def to_markdown(self) -> str:\n",
    "        lines = [\"# Reporte de Calidad de Datos\\n\"]\n",
    "        # Resumen tamaños\n",
    "        lines.append(\"## Resumen de filas por dataset\\n\")\n",
    "        for ds, meta in self.summary.items():\n",
    "            lines.append(f\"- **{ds}**: {meta.get('rows', 0):,} filas\")\n",
    "        lines.append(\"\\n---\\n\")\n",
    "        # Issues\n",
    "        if not self.issues:\n",
    "            lines.append(\"✅ No se encontraron problemas.\")\n",
    "            return \"\\n\".join(lines)\n",
    "\n",
    "        lines.append(\"## Hallazgos\\n\")\n",
    "        df = self.to_dataframe()\n",
    "        for _, row in df.sort_values([\"severity\",\"dataset\",\"check\"]).iterrows():\n",
    "            lines.append(f\"### [{row['severity']}] {row['dataset']} → {row['check']}\")\n",
    "            lines.append(f\"- **Impacto**: {row['n_affected']:,} filas ({row['pct_affected']*100:.4f}%)\")\n",
    "            lines.append(f\"- **Descripción**: {row['description']}\")\n",
    "            if row.get(\"suggestion\"):\n",
    "                lines.append(f\"- **Sugerencia**: `{row['suggestion']}`\")\n",
    "            if isinstance(row.get(\"sample_index\"), list) and row[\"sample_index\"]:\n",
    "                lines.append(f\"- **Muestra**: {row['sample_index'][:10]}\")\n",
    "            lines.append(\"\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Utilidades de chequeo\n",
    "# ==========================================================\n",
    "\n",
    "BOOLEAN_STRINGS_TRUE = {\"true\",\"t\",\"1\",\"yes\",\"y\"}\n",
    "BOOLEAN_STRINGS_FALSE = {\"false\",\"f\",\"0\",\"no\",\"n\"}\n",
    "\n",
    "def _pct(numer: int, denom: int) -> float:\n",
    "    if denom <= 0:\n",
    "        return 0.0\n",
    "    return numer / denom\n",
    "\n",
    "def check_required_columns(df: pd.DataFrame, required_cols: List[str]) -> Tuple[List[str], List[str]]:\n",
    "    present = [c for c in required_cols if c in df.columns]\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    return present, missing\n",
    "\n",
    "def check_duplicates(df: pd.DataFrame, key_cols: List[str]) -> pd.Series:\n",
    "    if not set(key_cols).issubset(df.columns):\n",
    "        return pd.Series(dtype=bool)\n",
    "    return df.duplicated(subset=key_cols, keep=False)\n",
    "\n",
    "def is_boolean_like_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Devuelve máscara booleana indicando qué valores SON interpretables como booleanos.\n",
    "    \"\"\"\n",
    "    if pd.api.types.is_bool_dtype(s):\n",
    "        return pd.Series(True, index=s.index)\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        return s.isna() | s.isin([0,1])\n",
    "    # objeto/cadena\n",
    "    lower = s.astype(str).str.lower()\n",
    "    return s.isna() | lower.isin(BOOLEAN_STRINGS_TRUE | BOOLEAN_STRINGS_FALSE)\n",
    "\n",
    "def coercible_numeric_mask(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    True si el valor puede convertirse a numérico; False si no.\n",
    "    \"\"\"\n",
    "    # Pandas to_numeric con errors='coerce' → NaN para no coercibles\n",
    "    as_num = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return ~as_num.isna() | s.isna()\n",
    "\n",
    "def outlier_mask_iqr(x: pd.Series, k: float = 3.0) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Marca outliers tipo IQR*k (por encima). Solo positivos (ventas/transacciones no deberían ser < -∞).\n",
    "    \"\"\"\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    q1 = x.quantile(0.25)\n",
    "    q3 = x.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    if pd.isna(iqr) or iqr == 0:\n",
    "        # fallback: percentil alto\n",
    "        thr = x.quantile(0.999)\n",
    "        return x > thr\n",
    "    thr = q3 + k * iqr\n",
    "    return x > thr\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Chequeos por dataset\n",
    "# ==========================================================\n",
    "\n",
    "def validate_ventas(ventas: pd.DataFrame, report: DQReport) -> None:\n",
    "    ds = \"ventas\"\n",
    "    report.set_dataset_size(ds, len(ventas))\n",
    "\n",
    "    # 1) Esquema y claves\n",
    "    present, missing = check_required_columns(ventas, config.SCHEMA_EXPECTED[ds][\"required_cols\"])\n",
    "    if missing:\n",
    "        report.add_issue(DQIssue(\n",
    "            ds, \"schema_required_cols\", \"ERROR\",\n",
    "            n_affected=len(missing), pct_affected=_pct(len(missing), len(present)+len(missing)),\n",
    "            description=f\"Faltan columnas requeridas: {missing}\",\n",
    "            suggestion=\"Revisar extracción/lectura del CSV (usecols/encoding).\",\n",
    "            extra={\"present\": present}\n",
    "        ))\n",
    "\n",
    "    # 2) Tipos/coercibilidad\n",
    "    for col in [\"unit_sales\"]:\n",
    "        mask = coercible_numeric_mask(ventas[col]) if col in ventas.columns else pd.Series(dtype=bool)\n",
    "        if col in ventas.columns and (~mask).any():\n",
    "            bad = (~mask).sum()\n",
    "            report.add_issue(DQIssue(\n",
    "                ds, f\"non_numeric_{col}\", \"ERROR\",\n",
    "                n_affected=int(bad), pct_affected=_pct(int(bad), len(ventas)),\n",
    "                description=f\"{col} contiene valores no numéricos o corruptos.\",\n",
    "                suggestion=f\"Coercer con pd.to_numeric({col}, errors='coerce') y revisar NaN resultantes.\"\n",
    "            ))\n",
    "\n",
    "    # 3) onpromotion interpretable\n",
    "    if \"onpromotion\" in ventas.columns:\n",
    "        ok_bool = is_boolean_like_series(ventas[\"onpromotion\"])\n",
    "        if (~ok_bool).any():\n",
    "            nbad = int((~ok_bool).sum())\n",
    "            report.add_issue(DQIssue(\n",
    "                ds, \"onpromotion_non_boolean_like\", \"WARNING\",\n",
    "                n_affected=nbad, pct_affected=_pct(nbad, len(ventas)),\n",
    "                description=\"Valores de 'onpromotion' no interpretables como booleanos.\",\n",
    "                suggestion=\"Normalizar con utils.safe_bool_to_int(ventas['onpromotion']).\"\n",
    "            ))\n",
    "\n",
    "    # 4) Duplicados (date, store_nbr, item_nbr)\n",
    "    key = config.SCHEMA_EXPECTED[ds][\"key\"]\n",
    "    dups_mask = check_duplicates(ventas, key)\n",
    "    if not dups_mask.empty and dups_mask.any():\n",
    "        dup_idx = ventas.loc[dups_mask, key].head(10).to_dict(\"records\")\n",
    "        report.add_issue(DQIssue(\n",
    "            ds, \"duplicate_rows\", \"ERROR\",\n",
    "            n_affected=int(dups_mask.sum()), pct_affected=_pct(int(dups_mask.sum()), len(ventas)),\n",
    "            description=f\"Duplicados por clave {key}. Puede sesgar agregaciones.\",\n",
    "            suggestion=\"Resolver sumando devoluciones/ventas o consolidando registros por clave.\",\n",
    "            sample_index=dup_idx\n",
    "        ))\n",
    "\n",
    "    # 5) Rango de fechas\n",
    "    if \"date\" in ventas.columns:\n",
    "        dmin, dmax = pd.to_datetime(ventas[\"date\"]).min(), pd.to_datetime(ventas[\"date\"]).max()\n",
    "        if dmin < pd.to_datetime(config.EXPECTED_DATE_MIN) or dmax > pd.to_datetime(config.EXPECTED_DATE_MAX):\n",
    "            report.add_issue(DQIssue(\n",
    "                ds, \"date_range_outside_expected\", \"WARNING\",\n",
    "                n_affected=1, pct_affected=0.0,\n",
    "                description=f\"Rango observado {dmin.date()}–{dmax.date()} fuera de [{config.EXPECTED_DATE_MIN.date()}–{config.EXPECTED_DATE_MAX.date()}].\",\n",
    "                suggestion=\"Verificar fuente/mezcla de datasets; filtrar al rango esperado si aplica.\"\n",
    "            ))\n",
    "\n",
    "    # 6) Ventas negativas extremas o sumas semanales que rompen log1p\n",
    "    #    (Si Y_isw < -1, log1p será NaN)\n",
    "    try:\n",
    "        temp = ventas[[\"date\",\"store_nbr\",\"item_nbr\",\"unit_sales\"]].copy()\n",
    "        temp[\"date\"] = pd.to_datetime(temp[\"date\"])\n",
    "        temp[\"unit_sales\"] = pd.to_numeric(temp[\"unit_sales\"], errors=\"coerce\")\n",
    "        temp = add_week_start(temp, \"date\", \"week_start\")\n",
    "        agg = (temp\n",
    "               .groupby([\"store_nbr\",\"item_nbr\",\"week_start\"], as_index=False)\n",
    "               .agg(Y_isw=(\"unit_sales\",\"sum\")))\n",
    "        bad_weekly = agg[agg[\"Y_isw\"] < -1]\n",
    "        if not bad_weekly.empty:\n",
    "            samp = bad_weekly.head(10)[[\"store_nbr\",\"item_nbr\",\"week_start\",\"Y_isw\"]].to_dict(\"records\")\n",
    "            report.add_issue(DQIssue(\n",
    "                ds, \"weekly_sum_lt_minus1\", \"ERROR\",\n",
    "                n_affected=int(len(bad_weekly)), pct_affected=_pct(len(bad_weekly), len(agg)),\n",
    "                description=\"Suma semanal de ventas menor a -1 (log1p generará NaN). Suele indicar devoluciones no compensadas.\",\n",
    "                suggestion=\"Separar devoluciones o truncar: Y_isw = max(Y_isw, 0) antes de log1p.\",\n",
    "                sample_index=samp\n",
    "            ))\n",
    "    except Exception as e:\n",
    "        report.add_issue(DQIssue(\n",
    "            ds, \"weekly_sum_check_failed\", \"INFO\",\n",
    "            n_affected=0, pct_affected=0.0,\n",
    "            description=f\"No se pudo evaluar sumas semanales (detalle: {type(e).__name__}).\",\n",
    "            suggestion=\"Revisar tipos de columnas 'date'/'unit_sales'.\"\n",
    "        ))\n",
    "\n",
    "    # 7) Outliers muy altos en unit_sales (potenciales errores de carga)\n",
    "    if \"unit_sales\" in ventas.columns:\n",
    "        mask_out = outlier_mask_iqr(ventas[\"unit_sales\"], k=6.0)\n",
    "        if mask_out.any():\n",
    "            report.add_issue(DQIssue(\n",
    "                ds, \"unit_sales_high_outliers\", \"WARNING\",\n",
    "                n_affected=int(mask_out.sum()), pct_affected=_pct(int(mask_out.sum()), len(ventas)),\n",
    "                description=\"Valores atípicos muy altos en 'unit_sales'.\",\n",
    "                suggestion=\"Winsorizar o revisar manualmente top casos.\",\n",
    "                sample_index=ventas.loc[mask_out, [\"date\",\"store_nbr\",\"item_nbr\",\"unit_sales\"]].head(10).to_dict(\"records\")\n",
    "            ))\n",
    "\n",
    "\n",
    "def validate_items(items: pd.DataFrame, report: DQReport) -> None:\n",
    "    ds = \"items\"\n",
    "    report.set_dataset_size(ds, len(items))\n",
    "\n",
    "    present, missing = check_required_columns(items, config.SCHEMA_EXPECTED[ds][\"required_cols\"])\n",
    "    if missing:\n",
    "        report.add_issue(DQIssue(\n",
    "            ds, \"schema_required_cols\", \"ERROR\",\n",
    "            n_affected=len(missing), pct_affected=_pct(len(missing), len(present)+len(missing)),\n",
    "            description=f\"Faltan columnas requeridas: {missing}\",\n",
    "            suggestion=\"Revisar extracción/lectura del CSV.\"\n",
    "        ))\n",
    "\n",
    "    dups = check_duplicates(items, config.SCHEMA_EXPECTED[ds][\"key\"])\n",
    "    if not dups.empty and dups.any():\n",
    "        report.add_issue(DQIssue(\n",
    "            ds, \"duplicate_keys\", \"ERROR\",\n",
    "            n_affected=int(dups.sum()), pct_affected=_pct(int(dups.sum()), len(items)),\n",
    "            description=\"item_nbr duplicado.\",\n",
    "            suggestion=\"Eliminar duplicados conservando el registro de mayor confianza.\"\n",
    "        ))\n",
    "\n",
    "    # Familias mínimas\n",
    "    if \"family\" in items.columns:\n",
    "        nfam = int(items[\"family\"].nunique(dropna=True))\n",
    "        if nfam < config.EXPECTED_MIN_FAMILIES:\n",
    "            report.add_issue(DQIssue(\n",
    "                ds, \"families_below_expected\", \"WARNING\",\n",
    "                n_affected=config.EXPECTED_MIN_FAMILIES - nfam, pct_affected=0.0,\n",
    "                description=f\"Se esperaban ≥{config.EXPECTED_MIN_FAMILIES} familias; hay {nfam}.\",\n",
    "                suggestion=\"Verificar mapeo de items a familias.\"\n",
    "            ))\n",
    "\n",
    "\n",
    "def validate_stores(stores: pd.DataFrame, report: DQReport) -> None:\n",
    "    ds = \"stores\"\n",
    "    report.set_dataset_size(ds, len(stores))\n",
    "\n",
    "    present, missing = check_required_columns(stores, config.SCHEMA_EXPECTED[ds][\"required_cols\"])\n",
    "    if missing:\n",
    "        report.add_issue(DQIssue(\n",
    "            ds, \"schema_required_cols\", \"ERROR\",\n",
    "            n_affected=len(missing), pct_affected=_pct(len(missing), len(present)+len(missing)),\n",
    "            description=f\"Faltan columnas requeridas: {missing}\",\n",
    "            suggestion=\"Revisar extracción/lectura del CSV.\"\n",
    "        ))\n",
    "\n",
    "    dups = check_duplicates(stores, config.SCHEMA_EXPECTED[ds][\"key\"])\n",
    "    if not dups.empty and dups.any():\n",
    "        report.add_issue(DQIssue(\n",
    "            ds, \"duplicate_keys\", \"ERROR\",\n",
    "            n_affected=int(dups.sum()), pct_affected=_pct(int(dups.sum()), len(stores)),\n",
    "            description=\"store_nbr duplicado.\",\n",
    "            suggestion=\"Eliminar o consolidar duplicados.\"\n",
    "        ))\n",
    "\n",
    "    # Nº de tiendas mínimo\n",
    "    nstores = int(stores[\"store_nbr\"].nunique()) if \"store_nbr\" in stores.columns else 0\n",
    "    if nstores < config.EXPECTED_MIN_STORES:\n",
    "        report.add_issue(DQIssue(\n",
    "            ds, \"stores_below_expected\", \"WARNING\",\n",
    "            n_affected=config.EXPECTED_MIN_STORES - nstores, pct_affected=0.0,\n",
    "            description=f\"Se esperaban ≥{config.EXPECTED_MIN_STORES} tiendas; hay {nstores}.\",\n",
    "            suggestion=\"Verificar archivo 'stores.csv'.\"\n",
    "        ))\n",
    "\n",
    "\n",
    "def validate_transactions(trans: pd.DataFrame, report: DQReport) -> None:\n",
    "    ds = \"trans\"\n",
    "    report.set_dataset_size(ds, len(trans))\n",
    "\n",
    "    present, missing = check_required_columns(trans, config.SCHEMA_EXPECTED[ds][\"required_cols\"])\n",
    "    if missing:\n",
    "        report.add_issue(DQIssue(\n",
    "            ds, \"schema_required_cols\", \"ERROR\",\n",
    "            n_affected=len(missing), pct_affected=_pct(len(missing), len(present)+len(missing)),\n",
    "            description=f\"Faltan columnas requeridas: {missing}\",\n",
    "            suggestion=\"Revisar extracción/lectura del CSV.\"\n",
    "        ))\n",
    "\n",
    "    dups = check_duplicates(trans, config.SCHEMA_EXPECTED[ds][\"key\"])\n",
    "    if not dups.empty and dups.any():\n",
    "        report.add_issue(DQIssue(\n",
    "            ds, \"duplicate_keys\", \"ERROR\",\n",
    "            n_affected=int(dups.sum()), pct_affected=_pct(int(dups.sum()), len(trans)),\n",
    "            description=\"Duplicados por (date, store_nbr) en transacciones.\",\n",
    "            suggestion=\"Consolidar por suma de 'transactions'.\"\n",
    "        ))\n",
    "\n",
    "    # Transacciones negativas/No numéricas\n",
    "    if \"transactions\" in trans.columns:\n",
    "        mask_num = coercible_numeric_mask(trans[\"transactions\"])\n",
    "        if (~mask_num).any():\n",
    "            report.add_issue(DQIssue(\n",
    "                ds, \"non_numeric_transactions\", \"ERROR\",\n",
    "                n_affected=int((~mask_num).sum()), pct_affected=_pct(int((~mask_num).sum()), len(trans)),\n",
    "                description=\"Valores no numéricos en 'transactions'.\",\n",
    "                suggestion=\"Coercer con pd.to_numeric(..., errors='coerce') y revisar NaN.\"\n",
    "            ))\n",
    "        neg = pd.to_numeric(trans[\"transactions\"], errors=\"coerce\") < 0\n",
    "        if neg.any():\n",
    "            report.add_issue(DQIssue(\n",
    "                ds, \"negative_transactions\", \"WARNING\",\n",
    "                n_affected=int(neg.sum()), pct_affected=_pct(int(neg.sum()), len(trans)),\n",
    "                description=\"Transacciones negativas detectadas.\",\n",
    "                suggestion=\"Revisar fuentes o truncar a cero si se justifica.\"\n",
    "            ))\n",
    "\n",
    "    # Cobertura por tienda (fechas faltantes)\n",
    "    try:\n",
    "        trans[\"date\"] = pd.to_datetime(trans[\"date\"])\n",
    "        expected_days = pd.date_range(config.EXPECTED_DATE_MIN, config.EXPECTED_DATE_MAX, freq=\"D\")\n",
    "        gaps = []\n",
    "        for s, grp in trans.groupby(\"store_nbr\", dropna=True):\n",
    "            present_days = pd.DatetimeIndex(grp[\"date\"].unique())\n",
    "            coverage = present_days.size / expected_days.size\n",
    "            if coverage < 0.9:  # umbral configurable\n",
    "                gaps.append((int(s), float(coverage)))\n",
    "        if gaps:\n",
    "            report.add_issue(DQIssue(\n",
    "                ds, \"store_date_coverage_low\", \"INFO\",\n",
    "                n_affected=len(gaps), pct_affected=_pct(len(gaps), trans[\"store_nbr\"].nunique()),\n",
    "                description=\"Tiendas con cobertura diaria < 90% en 'transactions'.\",\n",
    "                suggestion=\"Completar días faltantes con 0 o imputación si aplica.\",\n",
    "                sample_index=gaps[:10]\n",
    "            ))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "def validate_oil(oil: pd.DataFrame, report: DQReport) -> None:\n",
    "    ds = \"oil\"\n",
    "    report.set_dataset_size(ds, len(oil))\n",
    "\n",
    "    present, missing = check_required_columns(oil, config.SCHEMA_EXPECTED[ds][\"required_cols\"])\n",
    "    if missing:\n",
    "        report.add_issue(DQIssue(\n",
    "            ds, \"schema_required_cols\", \"ERROR\",\n",
    "            n_affected=len(missing), pct_affected=_pct(len(missing), len(present)+len(missing)),\n",
    "            description=f\"Faltan columnas requeridas: {missing}\",\n",
    "            suggestion=\"Revisar extracción/lectura del CSV.\"\n",
    "        ))\n",
    "\n",
    "    if \"dcoilwtico\" in oil.columns:\n",
    "        num = pd.to_numeric(oil[\"dcoilwtico\"], errors=\"coerce\")\n",
    "        miss = num.isna().sum()\n",
    "        if miss > 0:\n",
    "            report.add_issue(DQIssue(\n",
    "                ds, \"missing_oil_values\", \"WARNING\",\n",
    "                n_affected=int(miss), pct_affected=_pct(int(miss), len(oil)),\n",
    "                description=\"Valores faltantes en 'dcoilwtico' (fines de semana/feriados son comunes).\",\n",
    "                suggestion=\"Aplicar forward-fill antes de promediar semanalmente.\"\n",
    "            ))\n",
    "\n",
    "\n",
    "def validate_holidays(hol: pd.DataFrame, stores: pd.DataFrame, report: DQReport) -> None:\n",
    "    ds = \"hol\"\n",
    "    report.set_dataset_size(ds, len(hol))\n",
    "\n",
    "    present, missing = check_required_columns(hol, config.SCHEMA_EXPECTED[ds][\"required_cols\"])\n",
    "    if missing:\n",
    "        report.add_issue(DQIssue(\n",
    "            ds, \"schema_required_cols\", \"ERROR\",\n",
    "            n_affected=len(missing), pct_affected=_pct(len(missing), len(present)+len(missing)),\n",
    "            description=f\"Faltan columnas requeridas: {missing}\",\n",
    "            suggestion=\"Revisar extracción/lectura del CSV.\"\n",
    "        ))\n",
    "\n",
    "    # Valores válidos en 'locale'\n",
    "    if \"locale\" in hol.columns:\n",
    "        allowed = {\"National\",\"Regional\",\"Local\"}\n",
    "        invalid = ~hol[\"locale\"].isin(list(allowed))\n",
    "        if invalid.any():\n",
    "            report.add_issue(DQIssue(\n",
    "                ds, \"invalid_locale_values\", \"ERROR\",\n",
    "                n_affected=int(invalid.sum()), pct_affected=_pct(int(invalid.sum()), len(hol)),\n",
    "                description=\"Valores no válidos en 'locale'.\",\n",
    "                suggestion=\"Restringir a {'National','Regional','Local'}.\"\n",
    "            ))\n",
    "\n",
    "    # Eventos transferidos presentes (deberían excluirse corriente abajo)\n",
    "    if \"transferred\" in hol.columns:\n",
    "        ntrans = int((hol[\"transferred\"] == True).sum())\n",
    "        if ntrans > 0:\n",
    "            report.add_issue(DQIssue(\n",
    "                ds, \"contains_transferred\", \"INFO\",\n",
    "                n_affected=ntrans, pct_affected=_pct(ntrans, len(hol)),\n",
    "                description=\"Hay eventos con transferred=True (se recomienda excluirlos).\",\n",
    "                suggestion=\"hol = hol[hol['transferred']==False].copy()\"\n",
    "            ))\n",
    "\n",
    "    # Integridad con stores para Regional/Local\n",
    "    try:\n",
    "        issues = 0\n",
    "        # Regional\n",
    "        reg = hol[hol[\"locale\"]==\"Regional\"]\n",
    "        if not reg.empty and {\"locale_name\",\"state\"}.issubset(stores.columns.union({\"locale_name\"})):\n",
    "            valid_states = set(stores[\"state\"].dropna().unique())\n",
    "            bad_reg = ~reg[\"locale_name\"].isin(valid_states)\n",
    "            if bad_reg.any():\n",
    "                n = int(bad_reg.sum()); issues += n\n",
    "                report.add_issue(DQIssue(\n",
    "                    ds, \"regional_unmatched_state\", \"WARNING\",\n",
    "                    n_affected=n, pct_affected=_pct(n, len(reg)),\n",
    "                    description=\"Algunos 'locale_name' Regional no hacen match con stores.state.\",\n",
    "                    suggestion=\"Normalizar nombres de estado o diccionario de equivalencias.\",\n",
    "                    sample_index=reg.loc[bad_reg,\"locale_name\"].dropna().unique().tolist()[:10]\n",
    "                ))\n",
    "        # Local\n",
    "        loc = hol[hol[\"locale\"]==\"Local\"]\n",
    "        if not loc.empty and {\"locale_name\",\"city\"}.issubset(stores.columns.union({\"locale_name\"})):\n",
    "            valid_cities = set(stores[\"city\"].dropna().unique())\n",
    "            bad_loc = ~loc[\"locale_name\"].isin(valid_cities)\n",
    "            if bad_loc.any():\n",
    "                n = int(bad_loc.sum()); issues += n\n",
    "                report.add_issue(DQIssue(\n",
    "                    ds, \"local_unmatched_city\", \"WARNING\",\n",
    "                    n_affected=n, pct_affected=_pct(n, len(loc)),\n",
    "                    description=\"Algunos 'locale_name' Local no hacen match con stores.city.\",\n",
    "                    suggestion=\"Normalizar nombres de ciudad o diccionario de equivalencias.\",\n",
    "                    sample_index=loc.loc[bad_loc,\"locale_name\"].dropna().unique().tolist()[:10]\n",
    "                ))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Chequeos cruzados (integridad referencial)\n",
    "# ==========================================================\n",
    "\n",
    "def validate_cross_integrity(raw: Dict[str, pd.DataFrame], report: DQReport) -> None:\n",
    "    ventas = raw[\"ventas\"]; items = raw[\"items\"]; stores = raw[\"stores\"]\n",
    "    trans = raw[\"trans\"]\n",
    "\n",
    "    # ventas.item_nbr ⊆ items.item_nbr\n",
    "    if set([\"item_nbr\"]).issubset(ventas.columns) and \"item_nbr\" in items.columns:\n",
    "        missing_items = set(ventas[\"item_nbr\"].unique()) - set(items[\"item_nbr\"].unique())\n",
    "        if missing_items:\n",
    "            report.add_issue(DQIssue(\n",
    "                \"cross\", \"ventas_items_fk\", \"ERROR\",\n",
    "                n_affected=len(missing_items), pct_affected=_pct(len(missing_items), ventas[\"item_nbr\"].nunique()),\n",
    "                description=\"Existen item_nbr en ventas sin correspondencia en items.\",\n",
    "                suggestion=\"Revisar 'items.csv' o depurar registros huérfanos.\",\n",
    "                sample_index=list(sorted(list(missing_items)))[:10]\n",
    "            ))\n",
    "\n",
    "    # ventas.store_nbr ⊆ stores.store_nbr\n",
    "    if set([\"store_nbr\"]).issubset(ventas.columns) and \"store_nbr\" in stores.columns:\n",
    "        missing_stores = set(ventas[\"store_nbr\"].unique()) - set(stores[\"store_nbr\"].unique())\n",
    "        if missing_stores:\n",
    "            report.add_issue(DQIssue(\n",
    "                \"cross\", \"ventas_stores_fk\", \"ERROR\",\n",
    "                n_affected=len(missing_stores), pct_affected=_pct(len(missing_stores), ventas[\"store_nbr\"].nunique()),\n",
    "                description=\"Existen store_nbr en ventas sin correspondencia en stores.\",\n",
    "                suggestion=\"Revisar 'stores.csv' o depurar registros huérfanos.\",\n",
    "                sample_index=list(sorted(list(missing_stores)))[:10]\n",
    "            ))\n",
    "\n",
    "    # trans.store_nbr ⊆ stores.store_nbr\n",
    "    if set([\"store_nbr\"]).issubset(trans.columns) and \"store_nbr\" in stores.columns:\n",
    "        missing_stores_t = set(trans[\"store_nbr\"].unique()) - set(stores[\"store_nbr\"].unique())\n",
    "        if missing_stores_t:\n",
    "            report.add_issue(DQIssue(\n",
    "                \"cross\", \"trans_stores_fk\", \"ERROR\",\n",
    "                n_affected=len(missing_stores_t), pct_affected=_pct(len(missing_stores_t), trans[\"store_nbr\"].nunique()),\n",
    "                description=\"Existen store_nbr en trans sin correspondencia en stores.\",\n",
    "                suggestion=\"Revisar 'stores.csv' o depurar registros huérfanos.\",\n",
    "                sample_index=list(sorted(list(missing_stores_t)))[:10]\n",
    "            ))\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Orquestador\n",
    "# ==========================================================\n",
    "\n",
    "def run_all_validations(raw: Dict[str, pd.DataFrame]) -> DQReport:\n",
    "    \"\"\"\n",
    "    Ejecuta todas las validaciones y retorna un DQReport con issues y resumen.\n",
    "    \"\"\"\n",
    "    report = DQReport()\n",
    "\n",
    "    # Por dataset\n",
    "    validate_ventas(raw[\"ventas\"], report)\n",
    "    validate_items(raw[\"items\"], report)\n",
    "    validate_stores(raw[\"stores\"], report)\n",
    "    validate_transactions(raw[\"trans\"], report)\n",
    "    validate_oil(raw[\"oil\"], report)\n",
    "    validate_holidays(raw[\"hol\"], raw[\"stores\"], report)\n",
    "\n",
    "    # Integridad cruzada\n",
    "    validate_cross_integrity(raw, report)\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Persistencia de reportes\n",
    "# ==========================================================\n",
    "\n",
    "def save_report(report: DQReport, out_dir: Optional[str] = None) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Guarda el reporte en CSV y Markdown. Devuelve rutas generadas.\n",
    "    \"\"\"\n",
    "    if out_dir is None:\n",
    "        out_dir = config.REPORT_DIR\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    df = report.to_dataframe()\n",
    "    csv_path = os.path.join(out_dir, \"data_quality_report.csv\")\n",
    "    md_path  = os.path.join(out_dir, \"data_quality_report.md\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report.to_markdown())\n",
    "    return {\"csv\": csv_path, \"md\": md_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa85a863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_16560\\3624065105.py:2: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  \"ventas\": pd.read_csv(config.DATA_DIR + \"\\\\train.csv\"),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'csv': 'D:\\\\repos\\\\cannibalization_reatail\\\\.data\\\\raw_data\\\\data_quality_report.csv',\n",
       " 'md': 'D:\\\\repos\\\\cannibalization_reatail\\\\.data\\\\raw_data\\\\data_quality_report.md'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = {\n",
    "    \"ventas\": pd.read_csv(config.DATA_DIR + \"\\\\train.csv\"),\n",
    "    \"items\": pd.read_csv(config.DATA_DIR + \"\\\\items.csv\"),\n",
    "    \"stores\": pd.read_csv(config.DATA_DIR + \"\\\\stores.csv\"),\n",
    "    \"trans\": pd.read_csv(config.DATA_DIR + \"\\\\transactions.csv\"),\n",
    "    \"oil\": pd.read_csv(config.DATA_DIR + \"\\\\oil.csv\"),\n",
    "    \"hol\": pd.read_csv(config.DATA_DIR + \"\\\\holidays_events.csv\"),\n",
    "}\n",
    "\n",
    "report = run_all_validations(raw)\n",
    "save_report(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65344312",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
