# Estrategia de Experimentos - ComparaciÃ³n GSC vs Meta-Learners

## ðŸŽ¯ Objetivo

Obtener la **mÃ¡xima cantidad de episodios comparables** entre GSC (Generalized Synthetic Control) y Meta-Learners (T/S/X) para evaluar quÃ© algoritmo funciona mejor en diferentes configuraciones.

## âœ… Mejoras Aplicadas

### 1. **Todos los Experimentos Corren Ambos Algoritmos**
- âœ… **GSC**: Se ejecuta automÃ¡ticamente en todos los experimentos
- âœ… **Meta-Learners**: Ahora todos corren `["x", "s", "t"]` (3 learners)
- **Resultado**: Cada experimento genera 4 conjuntos de resultados comparables

### 2. **HPO Mejorado Propagado a Todos**
Todos los experimentos ahora incluyen:
```yaml
meta_hpo_trials: 100  # vs. 10 anterior
meta_max_iter: 1000   # vs. 500-600 anterior
```

### 3. **Experimentos RediseÃ±ados para ComparaciÃ³n**

| ID | DescripciÃ³n | Objetivo de ComparaciÃ³n |
|----|-------------|------------------------|
| **A_base** | Baseline con HPO mejorado | ConfiguraciÃ³n estÃ¡ndar optimizada |
| **B_donors30** | 30 donantes (vs. 20) | Â¿MÃ¡s donantes mejora GSC? Â¿Afecta Meta? |
| **C_donors5_hiqual** | 5 donantes alta calidad | Â¿Calidad > cantidad? Filtros estrictos |
| **D_seasonal_rich** | Fourier=6, lags hasta 84 dÃ­as | Â¿Captura mejor estacionalidad? |
| **E_no_stl** | Sin descomposiciÃ³n STL | Â¿STL es necesario o aÃ±ade ruido? |
| **F_treat_continuous** | Tratamiento continuo H_prop | Â¿Mejor que binarizado? Solo Meta puede usar esto |
| **G_cv_robust** | 5 folds, 35 dÃ­as holdout | Â¿CV mÃ¡s robusto reduce overfitting? |
| **H_gsc_rank8** | GSC rank=8, tau mÃ¡s bajo | Â¿GSC mÃ¡s flexible mejora ajuste? |

## ðŸ“Š Episodios Esperados por Experimento

Con el dataset aumentado (~100 canÃ­bales Ã— 50 vÃ­ctimas):

| Algoritmo | Episodios Esperados | Archivos de Salida |
|-----------|--------------------|--------------------|
| **GSC** | ~100-150 | `gsc_metrics.parquet` |
| **X-Learner** | ~100-150 | `meta_metrics_x.parquet` |
| **S-Learner** | ~100-150 | `meta_metrics_s.parquet` |
| **T-Learner** | ~100-150 | `meta_metrics_t.parquet` |
| **TOTAL** | **~400-600** | Por experimento |

**Total en 8 experimentos**: ~3,200-4,800 episodios procesados

## ðŸ” Preguntas de InvestigaciÃ³n por Experimento

### A_base (Baseline)
- Â¿CuÃ¡l es el RMSPE_pre de cada algoritmo?
- Â¿QuÃ© algoritmo tiene mejor ajuste en PRE?
- Â¿CuÃ¡l estima efectos causales mÃ¡s realistas?

### B_donors30 vs A_base
- Â¿30 donantes mejora el ajuste de GSC?
- Â¿Los meta-learners se benefician de mÃ¡s features?
- Â¿Hay overfitting con mÃ¡s donantes?

### C_donors5_hiqual vs A_base
- Â¿5 donantes de alta calidad superan a 20 promedio?
- Â¿Filtros estrictos reducen episodios procesables?
- Â¿QuÃ© algoritmo es mÃ¡s robusto con menos donantes?

### D_seasonal_rich vs A_base
- Â¿Fourier=6 captura mejor estacionalidad que 3?
- Â¿Lags largos (84 dÃ­as) mejoran predicciÃ³n?
- Â¿Hay trade-off entre complejidad y generalizaciÃ³n?

### E_no_stl vs A_base
- Â¿STL es necesario o aÃ±ade ruido?
- Â¿QuÃ© algoritmo depende mÃ¡s de STL?
- Â¿Fourier=6 compensa la falta de STL?

### F_treat_continuous vs A_base
- Â¿Tratamiento continuo (H_prop) es mejor que binario?
- Solo Meta puede usar esto â†’ Â¿ventaja sobre GSC?
- Â¿S/T-learners mejoran con tratamiento continuo?

### G_cv_robust vs A_base
- Â¿CV mÃ¡s robusto (5 folds, 35 dÃ­as) reduce overfitting?
- Â¿Mejora la generalizaciÃ³n a POST?
- Â¿Hay trade-off con tiempo de ejecuciÃ³n?

### H_gsc_rank8 vs A_base
- Â¿GSC rank=8 mejora ajuste vs. rank=5?
- Â¿Tau mÃ¡s bajo (1e-5) reduce regularizaciÃ³n excesiva?
- Â¿Meta-learners mantienen ventaja con GSC optimizado?

## ðŸ“ˆ MÃ©tricas de ComparaciÃ³n

Para cada episodio y algoritmo, comparar:

### Ajuste en PRE (Calidad del Modelo)
- **RMSPE_pre**: Root Mean Squared Percentage Error
- **MAE_pre**: Mean Absolute Error
- **RÂ²_pre**: Coeficiente de determinaciÃ³n
- **Bias_pre**: Sesgo sistemÃ¡tico

### Validez del Contrafactual
- **Placebo espacial**: Â¿Detecta correctamente no-efecto?
- **Placebo temporal**: Â¿Estable en perÃ­odos sin tratamiento?
- **Leave-One-Out**: Â¿Robusto a exclusiÃ³n de donantes?

### Efecto Causal Estimado
- **ATE (Average Treatment Effect)**: Efecto promedio
- **ATT (Average Treatment on Treated)**: Efecto en tratados
- **Intervalos de confianza**: Incertidumbre
- **Heterogeneidad**: VariaciÃ³n entre episodios

## ðŸš€ EjecuciÃ³n

```bash
# Ejecutar todos los experimentos (8 configuraciones)
python 01_run_sweep.py

# Tiempo estimado: 16-32 horas (2-4h por experimento)
# RecomendaciÃ³n: Ejecutar en servidor overnight
```

## ðŸ“ Estructura de Salidas

```
.data/processed_data/
â”œâ”€â”€ A_base/
â”‚   â”œâ”€â”€ gsc/gsc_metrics.parquet          # GSC
â”‚   â””â”€â”€ meta_outputs/
â”‚       â”œâ”€â”€ x/meta_metrics_x.parquet     # X-Learner
â”‚       â”œâ”€â”€ s/meta_metrics_s.parquet     # S-Learner
â”‚       â””â”€â”€ t/meta_metrics_t.parquet     # T-Learner
â”œâ”€â”€ B_donors30/
â”‚   â””â”€â”€ ...
â””â”€â”€ ... (C-H)

figures/
â”œâ”€â”€ A_base/
â”‚   â”œâ”€â”€ gsc/                             # GrÃ¡ficos GSC
â”‚   â””â”€â”€ meta/                            # GrÃ¡ficos Meta
â””â”€â”€ ... (C-H)
```

## ðŸ“Š AnÃ¡lisis Post-Experimentos

### 1. Consolidar MÃ©tricas
```python
import pandas as pd
from pathlib import Path

results = []
for exp in ["A_base", "B_donors30", "C_donors5_hiqual", ...]:
    # GSC
    gsc = pd.read_parquet(f".data/processed_data/{exp}/gsc/gsc_metrics.parquet")
    gsc["algorithm"] = "GSC"
    gsc["experiment"] = exp
    results.append(gsc)
    
    # Meta-learners
    for learner in ["x", "s", "t"]:
        meta = pd.read_parquet(f".data/processed_data/{exp}/meta_outputs/{learner}/meta_metrics_{learner}.parquet")
        meta["algorithm"] = f"Meta-{learner.upper()}"
        meta["experiment"] = exp
        results.append(meta)

df_all = pd.concat(results, ignore_index=True)
```

### 2. Comparar Algoritmos
```python
# RMSPE_pre por algoritmo y experimento
comparison = df_all.groupby(["experiment", "algorithm"])["rmspe_pre"].agg(["mean", "median", "std", "count"])

# Mejor algoritmo por experimento
best = df_all.loc[df_all.groupby(["experiment", "episode_id"])["rmspe_pre"].idxmin()]
best["algorithm"].value_counts()
```

### 3. Visualizar
```python
import seaborn as sns
import matplotlib.pyplot as plt

# Boxplot de RMSPE_pre
plt.figure(figsize=(14, 6))
sns.boxplot(data=df_all, x="experiment", y="rmspe_pre", hue="algorithm")
plt.xticks(rotation=45)
plt.title("RMSPE_pre por Experimento y Algoritmo")
plt.tight_layout()
plt.savefig("comparison_rmspe.png", dpi=300)
```

## âš ï¸ Consideraciones

### Tiempo de EjecuciÃ³n
- **Por experimento**: 2-4 horas
- **Total (8 experimentos)**: 16-32 horas
- **RecomendaciÃ³n**: Ejecutar en servidor o dejar overnight

### Recursos Computacionales
- **RAM**: 8-16 GB recomendado
- **CPU**: Multi-core beneficia Optuna (paralelizaciÃ³n)
- **Disco**: ~5-10 GB por experimento

### Episodios Fallidos
Algunos episodios pueden fallar por:
- Insuficientes datos en PRE/POST
- Donantes de baja calidad
- Convergencia de optimizaciÃ³n

**SoluciÃ³n**: Los algoritmos continÃºan con los episodios vÃ¡lidos

## ðŸŽ¯ Criterios de Ã‰xito

Un experimento es exitoso si:
1. âœ… Procesa >80% de episodios esperados
2. âœ… RMSPE_pre < 0.30 en promedio
3. âœ… Placebos no detectan efectos espurios
4. âœ… Efectos causales son interpretables

## ðŸ“ PrÃ³ximos Pasos

1. âœ… Ejecutar `python 01_run_sweep.py`
2. â³ Monitorear logs durante ejecuciÃ³n
3. ðŸ“Š Consolidar mÃ©tricas al finalizar
4. ðŸ” Analizar quÃ© algoritmo y configuraciÃ³n funciona mejor
5. ðŸ“ˆ Generar reporte comparativo
6. ðŸŽ¯ Seleccionar configuraciÃ³n Ã³ptima para producciÃ³n

---

**Ãšltima actualizaciÃ³n**: 2025-01-08  
**ConfiguraciÃ³n**: 8 experimentos Ã— 4 algoritmos = 32 configuraciones comparables
