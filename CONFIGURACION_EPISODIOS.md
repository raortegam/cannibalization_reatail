# Configuraci√≥n de Episodios - Resumen de Cambios

## üéØ Problema Identificado

El pipeline solo estaba generando **10 episodios** en total, cuando se esperaban ~100-150 episodios para comparaci√≥n robusta entre algoritmos.

### üîç Causa Ra√≠z

El par√°metro `MAX_EPISODES_FOR_DONORS` en `3. select_pairs_and_donors.py` estaba limitado a **10 episodios** por defecto.

## ‚úÖ Cambios Realizados

### 1. **`src/preprocess_data/3. select_pairs_and_donors.py`** (L√≠nea 240)

```python
# ANTES
MAX_EPISODES_FOR_DONORS = _env_int("SPD_MAX_EPISODES_FOR_DONORS", 10)

# DESPU√âS
MAX_EPISODES_FOR_DONORS = _env_int("SPD_MAX_EPISODES_FOR_DONORS", 150)
```

**Impacto:** Ahora se seleccionar√°n hasta 150 episodios para GSC (y por ende, para todo el pipeline).

### 2. **`pipeline_config.yaml`** - L√≠mites Removidos

```yaml
# ANTES
max_episodes: 50
gsc_max_episodes: 50
meta_max_episodes: 50
eda_alg_max_episodes_gsc: 40
eda_alg_max_episodes_meta: 40

# DESPU√âS
max_episodes: null              # Sin l√≠mite
gsc_max_episodes: null          # Sin l√≠mite
meta_max_episodes: null         # Sin l√≠mite
eda_alg_max_episodes_gsc: null  # Sin l√≠mite (plotea todos)
eda_alg_max_episodes_meta: null # Sin l√≠mite (plotea todos)
```

**Impacto:** Todos los pasos del pipeline procesar√°n y visualizar√°n todos los episodios disponibles.

## üìä Episodios Esperados Ahora

### Flujo Completo

```
Step 3: select_pairs_and_donors.py
‚îú‚îÄ‚îÄ Dataset Meta: ~5,000 episodios (100 can√≠bales √ó 50 v√≠ctimas)
‚îú‚îÄ‚îÄ Selecci√≥n GSC: 150 episodios (top por delta_abs)
‚îî‚îÄ‚îÄ episodes_index.parquet: ~150 episodios

Step 4: pre_algorithm.py
‚îú‚îÄ‚îÄ Procesa ~150 episodios
‚îî‚îÄ‚îÄ Aplica filtros de calidad ‚Üí ~100-150 episodios v√°lidos

Step 5: GSC
‚îî‚îÄ‚îÄ Procesa ~100-150 episodios

Step 6: Meta-learners (X, S, T)
‚îî‚îÄ‚îÄ Cada learner procesa ~100-150 episodios

EDA: EDA_algorithms.py
‚îî‚îÄ‚îÄ Plotea TODOS los episodios disponibles (~100-150 por algoritmo)
```

### Por Experimento

| Algoritmo | Episodios Esperados | Archivos Generados |
|-----------|--------------------|--------------------|
| **GSC** | ~100-150 | `gsc_metrics.parquet` + ~100-150 PNGs |
| **X-Learner** | ~100-150 | `meta_metrics_x.parquet` + ~100-150 PNGs |
| **S-Learner** | ~100-150 | `meta_metrics_s.parquet` + ~100-150 PNGs |
| **T-Learner** | ~100-150 | `meta_metrics_t.parquet` + ~100-150 PNGs |
| **TOTAL** | **~400-600** | Por experimento |

### En 8 Experimentos

```
Total episodios procesados: ~3,200-4,800
Total gr√°ficos generados: ~3,200-4,800 PNGs
Total m√©tricas: 32 archivos parquet (4 por experimento √ó 8)
```

## üîß Par√°metros Configurables

Si necesitas ajustar el n√∫mero de episodios, puedes modificar:

### 1. Variable de Entorno (Temporal)

```bash
# Windows
set SPD_MAX_EPISODES_FOR_DONORS=200
python 01_run_sweep.py --experiments experiments.yaml

# Linux/Mac
export SPD_MAX_EPISODES_FOR_DONORS=200
python 01_run_sweep.py --experiments experiments.yaml
```

### 2. C√≥digo Fuente (Permanente)

Editar l√≠nea 240 en `src/preprocess_data/3. select_pairs_and_donors.py`:

```python
MAX_EPISODES_FOR_DONORS = _env_int("SPD_MAX_EPISODES_FOR_DONORS", 150)  # Cambiar 150 por el valor deseado
```

### 3. L√≠mites por Paso (En `pipeline_config.yaml`)

```yaml
max_episodes: 100              # L√≠mite en Step 4
gsc_max_episodes: 100          # L√≠mite para GSC
meta_max_episodes: 100         # L√≠mite para Meta-learners
eda_alg_max_episodes_gsc: 50   # L√≠mite de ploteo GSC
eda_alg_max_episodes_meta: 50  # L√≠mite de ploteo Meta
```

## ‚ö†Ô∏è Consideraciones

### Tiempo de Ejecuci√≥n

Con 150 episodios:

| Paso | Tiempo Estimado |
|------|----------------|
| **Step 3** | ~30-45 min (genera episodios y donantes) |
| **Step 4** | ~20-30 min (preprocesamiento) |
| **Step 5 (GSC)** | ~30-60 min (150 episodios) |
| **Step 6 (Meta)** | ~2-3 horas (150 episodios √ó 3 learners √ó HPO) |
| **EDA** | ~15-30 min (genera ~600 gr√°ficos) |
| **TOTAL por experimento** | **~3-5 horas** |

**Total para 8 experimentos:** ~24-40 horas

### Espacio en Disco

| Tipo | Tama√±o Estimado |
|------|----------------|
| **Datos procesados** | ~500 MB - 1 GB por experimento |
| **Gr√°ficos PNG** | ~200-400 MB por experimento |
| **PDFs** | ~50-100 MB por experimento |
| **TOTAL por experimento** | ~750 MB - 1.5 GB |
| **TOTAL 8 experimentos** | **~6-12 GB** |

### Memoria RAM

- **M√≠nimo:** 8 GB
- **Recomendado:** 16 GB
- **√ìptimo:** 32 GB (para HPO paralelo)

## üéØ Estrategia de Selecci√≥n de Episodios

El par√°metro `EPISODE_SELECTION_STRATEGY` controla c√≥mo se seleccionan los 150 episodios:

```python
EPISODE_SELECTION_STRATEGY = "top_delta_abs"  # Opciones: top_delta_abs | random | first
```

### Estrategias Disponibles

1. **`top_delta_abs`** (ACTUAL): Selecciona episodios con mayor cambio absoluto en ventas
   - ‚úÖ Episodios m√°s interesantes (mayor efecto potencial)
   - ‚úÖ Mejor para validar algoritmos
   - ‚ö†Ô∏è Puede sesgar hacia casos extremos

2. **`random`**: Selecci√≥n aleatoria
   - ‚úÖ Muestra representativa
   - ‚úÖ Sin sesgo
   - ‚ö†Ô∏è Puede incluir episodios poco informativos

3. **`first`**: Primeros N episodios
   - ‚úÖ R√°pido y determinista
   - ‚ö†Ô∏è Puede tener sesgo temporal

## üìù Verificaci√≥n

Para verificar cu√°ntos episodios se generaron:

```bash
# Ver episodios en el √≠ndice
python -c "import pandas as pd; df = pd.read_parquet('.data/processed_data/A_base/episodes_index.parquet'); print(f'Total: {len(df)}')"

# Ver episodios procesados por GSC
python -c "import pandas as pd; df = pd.read_parquet('.data/processed_data/A_base/gsc/gsc_metrics.parquet'); print(f'GSC: {len(df)}')"

# Ver episodios procesados por Meta-X
python -c "import pandas as pd; df = pd.read_parquet('.data/processed_data/A_base/meta_outputs/x/meta_metrics_x.parquet'); print(f'Meta-X: {len(df)}')"
```

## üöÄ Pr√≥ximos Pasos

1. ‚úÖ Cambios aplicados
2. üîÑ Ejecutar pipeline: `python 01_run_sweep.py --experiments experiments.yaml`
3. üìä Verificar n√∫mero de episodios generados
4. üìà Analizar resultados con ~100-150 episodios por algoritmo
5. üéØ Comparar rendimiento entre configuraciones

---

**Fecha:** 2025-01-08  
**Cambios:** Aumentado l√≠mite de episodios de 10 ‚Üí 150  
**Impacto:** ~10x m√°s episodios para comparaci√≥n robusta
